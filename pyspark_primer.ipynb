{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pyspark_primer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOALWfi47naCfEthjRQCEgS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sims-23/PySparkLearning/blob/main/pyspark_primer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, download Java. Next installing Apache Spark 3.3.0 with Hadoop 3.3 and then unzipping it."
      ],
      "metadata": {
        "id": "rPppHZO9Df-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.3.0/spark-3.3.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.0-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "VqLxvfDFDDYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also need to install findspark which locates Spark on the system and imports it as a regular library. yay."
      ],
      "metadata": {
        "id": "BUeSFY03EVzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "ynh0YuPbEYNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the environment path so we can run Pyspark in the Colab environment."
      ],
      "metadata": {
        "id": "PVKS3bCwEsev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.0-bin-hadoop3\""
      ],
      "metadata": {
        "id": "FPL7mnS8LMpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Locate Spark in the system (if want to find where the location is, findspark.find())"
      ],
      "metadata": {
        "id": "2lRuYsJrLabx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "G4iCOnWrLZ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SparkContext lets our application access a Spark cluster"
      ],
      "metadata": {
        "id": "dGgzIiOWU8cW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "spark_conf = SparkConf()\\\n",
        "  .setAppName(\"YourTest\")\\\n",
        "  .setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext.getOrCreate(spark_conf)"
      ],
      "metadata": {
        "id": "4YsNtTP1h2zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "MlfrZhO_fBx6",
        "outputId": "42368b69-134d-4f52-8409-09df387babbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=YourTest>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b67ab68f64f1:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>YourTest</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of numbers is on the driver machine - driver because it's the machine that'll tell others to do. Spark will distribute data across multiple machines (or cluster of machines) which will do the processing for us. Cluster (distributed computing? - a group of machines dedicated to doing a specific task)"
      ],
      "metadata": {
        "id": "_T5ip891fp3H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums = list(range(0, 1000001))\n",
        "len(nums)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqf07CJ2fbTE",
        "outputId": "4ac7be1e-46d4-4d3f-961e-c04b253cf538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1000001"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributing nums and gives back a type of RDD. RDD is ..."
      ],
      "metadata": {
        "id": "JbKzkbk8CAIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums_rdd = sc.parallelize(nums)\n",
        "nums_rdd"
      ],
      "metadata": {
        "id": "KZCyJSBHgF47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7aa2f1fa-a994-438a-d909-86c71b90e1ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't want to use collect function at this moment.\n",
        "\n"
      ],
      "metadata": {
        "id": "94zlcYbGCbm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nums_rdd.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKCkuN9ICWjd",
        "outputId": "a1f522e2-c11a-4d7b-d5f1-b645df4a74c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 1, 2, 3, 4]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at applying a function to an RDD."
      ],
      "metadata": {
        "id": "-SWHbvG3DBf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squared_nums_rdd = nums_rdd.map(lambda x: x**2)\n",
        "squared_nums_rdd.take(5)\n",
        "\n",
        "pairs = squared_nums_rdd.map(lambda x: (x, len(str(x))))\n",
        "pairs.take(5)\n",
        "\n",
        "even_digits_pairs = pairs.filter(lambda x: x[1] % 2 == 0)\n",
        "even_digits_pairs.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjfVe0O8CiTq",
        "outputId": "4f3e851b-4bb4-4480-87ef-3f4670a358b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(16, 2), (25, 2), (36, 2), (49, 2), (64, 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the last thing, wanted to cover groupbys for Spark. Spark has notion of key value pairs (key is what we're grouping on, value is what we're using for our computation) so first need to switch each element in list for our task of computing the average of the even digits squares with the same number of digits"
      ],
      "metadata": {
        "id": "5dZcu5HPU3cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_even_pairs = even_digits_pairs.map(lambda x: (x[1], x[0]))\n",
        "reverse_even_pairs.take(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9WbEYrbDN5l",
        "outputId": "e127c9fb-bca3-4b20-966d-a7b17f03ca01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 16), (2, 25), (2, 36), (2, 49), (2, 64)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = reverse_even_pairs.groupByKey()\n",
        "grouped.take(5)\n",
        "\n",
        "#seeing what the first two groups look like\n",
        "visible_list_groups = grouped.map(lambda x: (x[0], list(x[1])))\n",
        "visible_list_groups.take(2)"
      ],
      "metadata": {
        "id": "Tv66qS6rWa_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking to average the groups now. However, this was a slow way to average."
      ],
      "metadata": {
        "id": "Ge7k93VSXQw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "averaged = grouped.map(lambda x: (x[0], sum(x[1])/len(x[1])))\n",
        "averaged.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQjl2zQOXU7Q",
        "outputId": "ee29a615-517c-494e-df03-e63a168cfc53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(2, 45.166666666666664),\n",
              " (4, 4675.5),\n",
              " (6, 471838.0),\n",
              " (8, 47204941.666666664),\n",
              " (10, 4720705565.0),\n",
              " (12, 472075391214.1667)]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}